<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chapter 3: Linear Models for Regression - PRML Solutions</title>

    <link rel="stylesheet" href="../css/style.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"></script>
    <script defer src="../js/math-config.js"></script>
</head>

<body>
    <div class="container">
        <a href="../index.html" class="back-link">‚Üê Back to Home</a>

        <header>
            <h1>Chapter 3: Linear Models for Regression</h1>
        </header>

        <div class="chapter-intro">
            <p>This chapter mostly deals with linear models for regression and their basis functions (a fancy way of
                saying function on 'x').
                This equation will come up a lot:
                \[
                y(x) = w_0 + \sum_{i=1}^N w_i\,\phi\!\left({x}\right)\quad (3.1)
                \]
                where 'N' is the total number of features and w_i is the weight associated to the i_th feature.
            </p>
        </div>

        <div class="chapter">
            <div class="problem">
                <h3>Problem 3.1</h3>

                <div class="context">
                    <p>Before looking at the problem, lets understand what's linear regression. It's just a linear
                        combination of our input variables</p>

                    \[
                    y(x) = w_0 + w_1 x_1 + w_2 x_2 + \dots + w_N x_N
                    \]
                    where y is our target, x are our input features and w are the weights associated.
                    <p>But not everything is <i>linear</i> in life, for ex: consider 2 features, location and whether
                        there's a swimming pool, I'd say the location matters more and should be given a special
                        consideration for purchasing a new home. So what we do now is we apply some sort of a
                        transformation on <strong>x</strong> :

                        \[
                        y(x) = w_0 + w_1 \phi_1\!\left({x_1}\right) + w_2 \phi_2\!\left({x_2}\right) + \dots + w_N
                        \phi_N\!\left({x_N}\right)
                        \]

                    </p>

                    <p>The above equation is the same as equation (3.1)</p>

                    <p>Let's go through the question and understand it step by step!!!!</p>
                </div>

                <div class="problem-statement">
                    <h4>Question</h4>
                    <p>Show that the 'tanh' and the logistic sigmoid functions are related by:</p>
                    \[
                    \tanh(a) = 2\,\sigma(2a) - 1
                    \]
                    <p>Hence show that a general linear combination of logistic sigmoid functions of the form:</p>
                    \[
                    y(x,w) = w_0 + \sum_{j=1}^M w_j\,\sigma\!\left(\frac{x-\mu_j}{s}\right) \quad (3.101)
                    \]
                    <p>is equivalent to a linear combination of \(\tanh\) functions of the form:</p>
                    \[
                    y(x,u) = u_0 + \sum_{j=1}^M u_j\,\tanh\!\left(\frac{x-\mu_j}{s}\right) \quad (3.102)
                    \]
                    <p>and find expressions relating the new parameters \(\{u_j\}\) to the original parameters
                        \(\{w_j\}\).</p>
                </div>

                <div class="solution">
                    <h4>Solution:</h4>

                    <div class="step">
                        <strong>Part 1: Proving the relationship between tanh and sigmoid</strong>
                        <p>We know that the logistic function:</p>
                        \[
                        \sigma(a) = \frac{1}{1 + e^{-a}}
                        \]
                        <p>and the RHS can be written as</p>
                        \[
                        2 . \frac{1}{1 + e^{-2a}} - 1 => \frac{2e^{2a}}{1 + e^{2a}} - 1
                        \]
                        <p>solving the above equation gives us</p>
                        \[
                        \frac{e^{2a} - 1}{e^{2a} + 1}
                        \]
                        <p>which is nothing but \(\tanh(a)\) (a different form though)</p>
                    </div>

                    <div class="step">
                        <strong>Part 2: Converting sigmoid combination to tanh combination</strong>
                        <p>We can rewrite the relationship as:</p>
                        \[
                            \sigma(a) = \frac{\tanh\left(\frac{a}{2}\right) + 1}{2}
                        \]
                        <p>Substitute this relation into equation (3.101):</p>
                        \[
                        \begin{aligned}
                        y(x,w) &= w_0 + \sum_{j=1}^M w_j\,\sigma\!\left(\frac{x-\mu_j}{s}\right) \\
                        &= w_0 + \sum_{j=1}^M w_j\cdot\frac{1}{2}\left[\tanh\!\left(\frac{x-\mu_j}{2s}\right) + 1\right]
                        \\
                        &= \left(w_0 + \frac{1}{2}\sum_{j=1}^M w_j\right) + \sum_{j=1}^M
                        \frac{w_j}{2}\,\tanh\!\left(\frac{x-\mu_j}{2s}\right)
                        \end{aligned}
                        \]
                    <p>which aligns to eqn (3.102) (not sure if there's a printing mistake but you get the idea)</p>
                    </div>

                    <div class="note">
                        <strong>üí° Key Insight:</strong> The sigmoid and the hyperbolic tangent are affine transforms of
                        each other (up to a scale change in the argument), so linear combinations of one family can
                        always be expressed as linear combinations of the other by simple rescaling of parameters. This
                        shows that neural networks using sigmoid vs tanh activations are fundamentally equivalent in
                        their representational power.
                    </div>
                </div>
            </div>

            <!-- Template for next problem -->
            <div class="problem">
                <h3>Problem 3.2</h3>

                <div class="context">
                    <h4>üìö Context & Background</h4>
                    <p>[Explain the concepts needed to understand this problem...]</p>
                </div>

                <div class="problem-statement">
                    <h4>üìù Problem Statement</h4>
                    <p>[State the problem clearly...]</p>
                </div>

                <div class="solution">
                    <h4>‚úÖ Solution:</h4>

                    <div class="step">
                        <strong>Step 1:</strong>
                        <p>[Your solution...]</p>
                    </div>
                </div>
            </div>

        </div>

        <footer>
            <p><a href="../index.html">Home</a> | <a href="chapter4.html">Next Chapter ‚Üí</a></p>
            <p style="margin-top: 10px;">Created with ‚ù§Ô∏è for the ML community</p>
        </footer>
    </div>
</body>

</html>