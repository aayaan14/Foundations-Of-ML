<!-- chapter1.html -->

<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chapter 1: Linear Models for Regression - PRML Solutions</title>

    <link rel="stylesheet" href="../css/style.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"></script>
    <script defer src="../js/math-config.js"></script>
</head>

<body>
    <div class="container">
        <a href="../index.html" class="back-link">‚Üê Back to Home</a>

        <header>
            <h1>Chapter 1: Linear Models for Regression</h1>
        </header>

        <div class="chapter-intro">
            <p>This chapter mostly deals with linear models for regression and their basis functions (a fancy way of
                saying function on 'x').
                This equation will come up a lot:
                \[
                y(x) = w_0 + \sum_{i=1}^N w_i\,\phi\!\left({x}\right)\quad (1.1)
                \]
                where 'N' is the total number of features and w_i is the weight associated to the i_th feature.
            </p>
        </div>

        <div class="chapter">
            <!-- 1 -->
            <div class="problem">
                <h3>Problem 1.1</h3>

                <div class="context">
                    <h4>Context</h4>
                    <p>
                        Suppose we are asked to model the relationship between apartment size (in sq.ft)
                        and price for a Brigade apartment project in Bangalore.
                    </p>
                    <table class="data-table">
                        <thead>
                            <tr>
                                <th>Flat ID</th>
                                <th>Size x (sq.ft)</th>
                                <th>Total Price (‚Çπ Lakhs)</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td>F1</td>
                                <td>650</td>
                                <td>68</td>
                            </tr>
                            <tr>
                                <td>F2</td>
                                <td>800</td>
                                <td>82</td>
                            </tr>
                            <tr>
                                <td>F3</td>
                                <td>1100</td>
                                <td>105</td>
                            </tr>
                            <tr>
                                <td>F4</td>
                                <td>1400</td>
                                <td>128</td>
                            </tr>
                            <tr>
                                <td>F5</td>
                                <td>1800</td>
                                <td>167</td>
                            </tr>
                            <tr>
                                <td>F6</td>
                                <td>2200</td>
                                <td>215</td>
                            </tr>
                        </tbody>
                    </table>
                    <p>
                        A simple straight-line model may not be sufficient to describe this data
                        accurately. In real-world, the relationship between size
                        and price is often nonlinear due to location effects, floor level, amenities,
                        and demand variations across different size ranges etc etc.
                        The below equation is a general expression of what we're trying to achieve:
                    </p>
                    <img src="../assets/LR.png" alt="Linear Regression Illustration" class="context-image"
                        style="max-width: 100%; height: auto; display: block; margin: 0 auto;" loading="lazy">
                </div>

                <div class="problem-statement">
                    <h4>Question</h4>
                    <p>Show that the 'tanh' and the logistic sigmoid functions are related by:</p>
                    \[
                    \tanh(a) = 2\,\sigma(2a) - 1
                    \]
                    <p>Hence show that a general linear combination of logistic sigmoid functions of the form:</p>
                    \[
                    y(x,w) = w_0 + \sum_{j=1}^M w_j\,\sigma\!\left(\frac{x-\mu_j}{s}\right) \quad (1.101)
                    \]
                    <p>is equivalent to a linear combination of \(\tanh\) functions of the form:</p>
                    \[
                    y(x,u) = u_0 + \sum_{j=1}^M u_j\,\tanh\!\left(\frac{x-\mu_j}{s}\right) \quad (1.102)
                    \]
                    <p>and find expressions relating the new parameters \(\{u_j\}\) to the original parameters
                        \(\{w_j\}\).</p>
                </div>

                <div class="solution">
                    <h4>Solution:</h4>

                    <div class="step">
                        <strong>Part 1: Proving the relationship between tanh and sigmoid</strong>
                        <p>We know that the logistic function:</p>
                        \[
                        \sigma(a) = \frac{1}{1 + e^{-a}}
                        \]
                        <p>and the RHS can be written as</p>
                        \[
                        2 . \frac{1}{1 + e^{-2a}} - 1 => \frac{2e^{2a}}{1 + e^{2a}} - 1
                        \]
                        <p>solving the above equation gives us</p>
                        \[
                        \frac{e^{2a} - 1}{e^{2a} + 1}
                        \]
                        <p>which is nothing but \(\tanh(a)\) (a different form though)</p>
                    </div>

                    <div class="step">
                        <strong>Part 2: Converting sigmoid combination to tanh combination</strong>
                        <p>We can rewrite the relationship as:</p>
                        \[
                        \sigma(a) = \frac{\tanh\left(\frac{a}{2}\right) + 1}{2}
                        \]
                        <p>Substitute this relation into equation (1.101):</p>
                        \[
                        \begin{aligned}
                        y(x,w) &= w_0 + \sum_{j=1}^M w_j\,\sigma\!\left(\frac{x-\mu_j}{s}\right) \\
                        &= w_0 + \sum_{j=1}^M w_j\cdot\frac{1}{2}\left[\tanh\!\left(\frac{x-\mu_j}{2s}\right) + 1\right]
                        \\
                        &= \left(w_0 + \frac{1}{2}\sum_{j=1}^M w_j\right) + \sum_{j=1}^M
                        \frac{w_j}{2}\,\tanh\!\left(\frac{x-\mu_j}{2s}\right)
                        \end{aligned}
                        \]
                        <p>which aligns to eqn (1.102) (not sure if there's a printing mistake but you get the idea)</p>
                    </div>

                    <div class="note">
                        <strong>üí° Key Insight:</strong> The sigmoid and the hyperbolic tangent are affine transforms of
                        each other (up to a scale change in the argument), so linear combinations of one family can
                        always be expressed as linear combinations of the other by simple rescaling of parameters. This
                        shows that neural networks using sigmoid vs tanh activations are fundamentally equivalent in
                        their representational power.
                    </div>
                </div>
            </div>

            <!-- 2 -->
            <div class="problem">
                <h3>Problem 1.2</h3>

                <div class="context">
                    <h4>Context</h4>

                    <p>Continuing from our Brigade apartment example, here's how the actual data looks like
                    </p>

                    <img src="../assets/target_var.png" alt="Linear Regression Illustration" class="context-image"
                        style="max-width: 100%; height: auto; display: block; margin: 0 auto;" loading="lazy">

                    <p>The above graph represents a real life scenario where there's always a noise associated with the
                        target variable. This can be generalised by the below equation:</p>
                    \[
                    t = y(x, \mathbf{w}) + \epsilon
                    \]

                    <p>where \(\epsilon\) represents the Gaussian noise (adding it so we can generalise the equation).
                    </p>

                    <p>In order to learn the spread or uncertainity (as \(y(x, \mathbf{w})\) will just predict a value
                        but we're more interested in the distribution), we'll calculate the probability of the target
                        variable given the input and parameters.</p>

                    \[
                    p(t_n|x_n, \mathbf{w}, \beta) = \mathcal{N}(t_n|y(x_n, \mathbf{w}), \beta^{-1})
                    \]

                    <p>where \(\beta\) is the precision (inverse of the variance) of the Gaussian noise.</p>

                    <p>Now assuming all the data points are independent, likelihood is:</p>

                    \[
                    p(\mathbf{t}|\mathbf{X}, \mathbf{w}, \beta) = \prod_{n=1}^{N} \mathcal{N}(t_n|y(x_n, \mathbf{w}),
                    \beta^{-1})
                    \]

                    <p>where \(\mathbf{t}\) is the column vector representing the target values for all data points.</p>

                    <p>Taking the logarithm of the likelihood function, we get the log-likelihood:</p>

                    \[
                    \ln p(\mathbf{t}|\mathbf{X}, \mathbf{w}, \beta) = \frac{N}{2}\ln \beta - \frac{N}{2}\ln(2\pi) -
                    \beta E_D(\mathbf{w}) \quad
                    \]

                    <p>where \(E_D(\mathbf{w})\) is the sum-of-squares error function defined as:</p>

                    \[
                    E_D(\mathbf{w}) = \frac{1}{2}\sum_{n=1}^{N} \left\{ t_n - \mathbf{w}^T \phi(\mathbf{x}_n) \right\}^2
                    \]

                    <p>Maximizing the log-likelihood is equivalent to minimizing the sum-of-squares error function.</p>

                    \[
                    E(\mathbf{w})
                    = \frac{1}{2}\|\mathbf{t}-\Phi\mathbf{w}\|^2
                    = \frac{1}{2}(\mathbf{t}-\Phi\mathbf{w})^T(\mathbf{t}-\Phi\mathbf{w})
                    \]

                    \[
                    \ln p(\mathbf{t}|\mathbf{w},\beta)
                    = \text{const} - \beta E(\mathbf{w})
                    \]

                    \[
                    \nabla_{\mathbf{w}} \ln p
                    = -\beta \nabla_{\mathbf{w}} E(\mathbf{w}) = 0
                    \]

                    \[
                    \nabla_{\mathbf{w}} E(\mathbf{w})
                    = \nabla_{\mathbf{w}} \left[
                    \frac{1}{2}(\mathbf{t}-\Phi\mathbf{w})^T(\mathbf{t}-\Phi\mathbf{w})
                    \right]
                    \]

                    \[
                    \nabla_{\mathbf{w}} E(\mathbf{w})
                    = -\Phi^T(\mathbf{t}-\Phi\mathbf{w})
                    \]

                    \[
                    \Rightarrow -\Phi^T\mathbf{t} + \Phi^T\Phi\mathbf{w} = 0
                    \]

                    \[
                    \Rightarrow \Phi^T\Phi\mathbf{w} = \Phi^T\mathbf{t}
                    \]

                    \[
                    \Rightarrow \mathbf{w} = (\Phi^T\Phi)^{-1}\Phi^T\mathbf{t}
                    \]
                </div>

                <div class="problem-statement">
                    <h4>Question</h4>
                    <p>Show that the matrix
                        \[
                        H = \Phi(\Phi^{T}\Phi)^{-1}\Phi^{T} \tag{1.103}
                        \]
                        takes any vector \( \mathbf{v} \) and projects it onto the space spanned by the columns of \(
                        \Phi \).
                        Use this result to show that the least-squares solution
                        \[
                        \mathbf{y} = \Phi\mathbf{w}_{\text{LS}} = \Phi(\Phi^{T}\Phi)^{-1}\Phi^{T}\mathbf{t} \
                        \]
                        corresponds to an orthogonal projection of the vector \( \mathbf{t} \) onto the manifold \(
                        \mathcal{S} \).
                    </p>
                </div>

                <div class="solution">
                    <h4>Solution:</h4>

                    <div class="step">
                        <strong>Part 1: The Projection Property</strong>
                        <p>Let's define our matrix as \(H\) (the Hat Matrix). If we multiply it by any arbitrary vector
                            \(\mathbf{v}\):</p>
                        \[
                        \mathbf{u} = H\mathbf{v} = \Phi \underbrace{\left[ (\Phi^T\Phi)^{-1}\Phi^T \mathbf{v}
                        \right]}_{\text{some vector } \mathbf{a}}
                        \]
                        <p>The result \(\mathbf{u}\) is equal to \(\Phi \mathbf{a}\). By definition, any vector \(\Phi
                            \mathbf{a}\) is a linear combination of the columns of \(\Phi\). Therefore, \(H\) maps any
                            vector into the column space of \(\Phi\).</p>
                    </div>

                    <div class="step">
                        <strong>Part 2: Orthogonality of the Least Squares Solution</strong>
                        <p>The least squares prediction is \(\mathbf{y} = H\mathbf{t}\). The residual (error) vector is:
                        </p>
                        \[
                        \mathbf{r} = \mathbf{t} - \mathbf{y} = (I - H)\mathbf{t}
                        \]
                        <p>For the projection to be <strong>orthogonal</strong>, the residual \(\mathbf{r}\) must be
                            perpendicular to the subspace (the columns of \(\Phi\)). We check the dot product \(\Phi^T
                            \mathbf{r}\):</p>
                        \[
                        \begin{aligned}
                        \Phi^T \mathbf{r} &= \Phi^T (\mathbf{t} - \Phi(\Phi^T\Phi)^{-1}\Phi^T \mathbf{t}) \\
                        &= \Phi^T \mathbf{t} - (\Phi^T\Phi)(\Phi^T\Phi)^{-1}\Phi^T \mathbf{t} \\
                        &= \Phi^T \mathbf{t} - I \cdot \Phi^T \mathbf{t} \\
                        &= 0
                        \end{aligned}
                        \]
                        <p>Since the dot product is zero, the residual is orthogonal to the column space.</p>

                        <img src="../assets/error_vec.png" alt="Linear Regression Illustration" class="context-image"
                            style="max-width: 100%; height: auto; display: block; margin: 0 auto;" loading="lazy">
                    </div>

                    <div class="note">
                        <strong>üí° Key Insight:</strong> This proves that minimizing the sum-of-squares error (algebraic
                        view) is identical to finding the orthogonal projection of the data vector onto the model
                        subspace (geometric view).
                    </div>
                </div>
            </div>

            <!-- 3 -->
            <div class="problem">
                <h3>Problem 1.3</h3>

                <div class="context">
                    <h4>Context</h4>
                    <p>In the previous problem (1.2), we assumed that every data point (apartment price) was equally
                        reliable. However, in real-world scenarios, this is rarely the case.</p>

                    <p>For example, in our Brigade apartment dataset:</p>
                    <p> -> A price from a verified government sale deed is highly reliable (High weight).</p>
                    <p> -> A price from an anonymous online posting might be noisy or inaccurate (Low weight).</p>



                    <p>To handle this, we assign a "weight" \(r_n\) to each data point. If a point is reliable, \(r_n\)
                        is large; if it's noisy, \(r_n\) is small. This technique is known as <strong>Weighted Least
                            Squares</strong>.</p>
                </div>

                <div class="problem-statement">
                    <h4>Question</h4>
                    <p>Consider a data set in which each data point \(t_n\) is associated with a weighting
                        factor \(r_n > 0\), so that the sum-of-squares error function becomes
                    </p>
                    \[
                    E_D(\mathbf{w})
                    = \frac{1}{2}\sum_{n=1}^{N}
                    r_n \left\{ t_n - \mathbf{w}^T \phi(\mathbf{x}_n) \right\}^2
                    \]

                    <p>Find an expression for the solution \(\mathbf{w}^*\) that minimizes this error function.
                        Give two alternative interpretations of the weighted sum-of-squares error function
                        in terms of: (i) data-dependent noise variance and (ii) replicated data points.</p>
                </div>

                <div class="solution">
                    <h4>Solution:</h4>

                    <div class="step">
                        <strong>Part 1: Finding the Solution \(\mathbf{w}^*\)</strong>
                        <p>First, it is helpful to express the error function in matrix notation. We introduce a
                            diagonal
                            matrix \(\mathbf{R}\) of size \(N \times N\) containing the weights:</p>
                        \[
                        \mathbf{R} = \text{diag}(r_1, r_2, \dots, r_N)
                        \]
                        <p>The weighted sum-of-squares error can now be written as:</p>
                        \[
                        E_D(\mathbf{w}) = \frac{1}{2} (\mathbf{t} - \Phi\mathbf{w})^T \mathbf{R} (\mathbf{t} -
                        \Phi\mathbf{w})
                        \]
                        <p>To find the minimizer \(\mathbf{w}^*\), we set the gradient with respect to \(\mathbf{w}\) to
                            zero:</p>
                        \[
                        \nabla_{\mathbf{w}} E_D(\mathbf{w}) = -\Phi^T \mathbf{R} (\mathbf{t} - \Phi\mathbf{w}) = 0
                        \]
                        <p>Rearranging the terms to solve for \(\mathbf{w}\):</p>
                        \[
                        \Phi^T \mathbf{R} \Phi \mathbf{w} = \Phi^T \mathbf{R} \mathbf{t}
                        \]
                        \[
                        \mathbf{w}^* = (\Phi^T \mathbf{R} \Phi)^{-1} \Phi^T \mathbf{R} \mathbf{t}
                        \]
                        <p>This is the solution for Weighted Least Squares. Note that if \(\mathbf{R} = \mathbf{I}\)
                            (identity matrix), this reduces to the standard least squares solution found in Problem 1.2.
                        </p>
                    </div>

                    <div class="step">
                        <strong>Part 2: Interpretation (i) - Data-Dependent Noise Variance</strong>
                        <p>Recall that the standard sum-of-squares error comes from maximizing the likelihood of a
                            Gaussian distribution with constant variance \(\sigma^2\).</p>
                        <p>If we assume that each data point \(t_n\) has its own specific variance \(\sigma_n^2\), the
                            likelihood function becomes:</p>
                        \[
                        p(\mathbf{t}|\mathbf{w}) = \prod_{n=1}^{N} \mathcal{N}(t_n | \mathbf{w}^T \phi(\mathbf{x}_n),
                        \sigma_n^2)
                        \]
                        <p>Taking the negative log-likelihood, we isolate the error term:</p>
                        \[
                        \frac{1}{2} \sum_{n=1}^{N} \frac{1}{\sigma_n^2} (t_n - \mathbf{w}^T \phi(\mathbf{x}_n))^2
                        \]
                        <p>By comparing this to our weighted error function, we can see that:</p>
                        \[
                        r_n = \frac{1}{\sigma_n^2} \quad \text{(or } r_n \propto \beta_n \text{)}
                        \]
                        <p><strong>Interpretation:</strong> The weight \(r_n\) represents the precision of the
                            measurement. Points with high uncertainty (high variance) are given low weights, and points
                            with high precision are given high weights.</p>
                    </div>

                    <div class="step">
                        <strong>Part 3: Interpretation (ii) - Replicated Data Points</strong>
                        <p>Consider the case where all weights \(r_n\) are integers. The term:</p>
                        \[
                        r_n \left\{ t_n - \mathbf{w}^T \phi(\mathbf{x}_n) \right\}^2
                        \]
                        <p>is mathematically equivalent to:</p>
                        \[
                        \underbrace{\left\{ t_n - \mathbf{w}^T \phi(\mathbf{x}_n) \right\}^2 + \dots + \left\{ t_n -
                        \mathbf{w}^T \phi(\mathbf{x}_n) \right\}^2}_{r_n \text{ times}}
                        \]
                        <p><strong>Interpretation:</strong> We can view weighted least squares as a standard least
                            squares problem on a dataset where the \(n\)-th observation has been replicated \(r_n\)
                            times. This gives that specific data point more "voting power" in determining the model
                            parameters.</p>
                    </div>

                    <div class="note">
                        <strong>üí° Key Insight:</strong> Weighted Least Squares (WLS) is the bridge between standard
                        regression and probabilistic regression where data quality varies. It allows the model to
                        "listen" more closely to high-quality data while ignoring noisy outliers.
                    </div>
                </div>
            </div>

            <!-- 4 -->
            <div class="problem">
                <h3>Problem 1.4</h3>

                <div class="context">
                </div>

                <div class="problem-statement">
                </div>

                <div class="solution">
                </div>
            </div>

            <footer>
                <p><a href="../index.html">Home</a> | <a href="chapter2.html">Next Chapter ‚Üí</a></p>
                <p style="margin-top: 10px;">Created with ‚ù§Ô∏è for the ML community</p>
            </footer>
        </div>
</body>

</html>